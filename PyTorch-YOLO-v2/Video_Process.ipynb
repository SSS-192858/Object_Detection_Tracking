{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinivasan M\\AppData\\Local\\Temp\\ipykernel_10344\\2912876732.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = cv2.resize(orig_im, (inp_dim, inp_dim))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(x, img, classes, colors):\n",
    "    c1 = (int(x[1]), int(x[2]))  # Convert to integers\n",
    "    c2 = (int(x[3]), int(x[4]))  # Convert to integers\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = random.choice(colors)\n",
    "    cv2.rectangle(img, c1, c2, color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "    c2 = (c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4)\n",
    "    cv2.rectangle(img, c1, c2, color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='YOLO v2 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"video.avi\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinivasan M\\AppData\\Local\\Temp\\ipykernel_10344\\3651735613.py:61: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  output = model(Variable(img.to(\"cuda:0\" if CUDA else \"cpu\"), volatile=True)).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  1.09\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  1.91\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  2.57\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  3.20\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  3.74\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  4.22\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  4.63\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  4.78\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  4.97\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  4.99\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  5.06\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  5.12\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  5.25\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([3, 8])\n",
      "FPS of the video is  5.16\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([2, 8])\n",
      "FPS of the video is  5.04\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([2, 8])\n",
      "FPS of the video is  4.97\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([2, 8])\n",
      "FPS of the video is  4.63\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n",
      "torch.Size([2, 8])\n",
      "FPS of the video is  4.72\n",
      "Shape of output tensor: torch.Size([1, 845, 25])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 79\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;66;03m# output = write_results(output, num_classes, nms=True, nms_conf=nms_thresh)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Check shape of output tensor\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of output tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 79\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnms_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnms_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Srinivasan M\\OneDrive\\Desktop\\Semester_6\\VR\\Object_Detection_Tracking\\PyTorch-YOLO-v2\\util.py:164\u001b[0m, in \u001b[0;36mwrite_results\u001b[1;34m(prediction, num_classes, nms, nms_conf)\u001b[0m\n\u001b[0;32m    159\u001b[0m image_pred_class \u001b[38;5;241m=\u001b[39m image_pred_[class_mask_ind]\n\u001b[0;32m    162\u001b[0m  \u001b[38;5;66;03m#sort the detections such that the entry with the maximum objectness\u001b[39;00m\n\u001b[0;32m    163\u001b[0m  \u001b[38;5;66;03m#confidence is at the top\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m conf_sort_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(\u001b[43mimage_pred_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m, descending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m )[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    165\u001b[0m image_pred_class \u001b[38;5;241m=\u001b[39m image_pred_class[conf_sort_index]\n\u001b[0;32m    166\u001b[0m idx \u001b[38;5;241m=\u001b[39m image_pred_class\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# Assuming CUDA is defined somewhere in your code\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "nms_thresh = 0.4\n",
    "confidence = 0.5\n",
    "\n",
    "dataset_train = \"pascal\"\n",
    "\n",
    "if dataset_train == \"pascal\":\n",
    "    inp_dim = 416\n",
    "    num_classes = 20\n",
    "    classes = load_classes('data/voc.names')\n",
    "    weightsfile = 'yolo-voc.weights'\n",
    "    cfgfile = \"cfg/yolo-voc.cfg\"\n",
    "\n",
    "elif dataset_train == \"coco\":\n",
    "    inp_dim = 544\n",
    "    num_classes = 80\n",
    "    classes = load_classes('data/coco.names')\n",
    "    weightsfile = 'yolo.weights'\n",
    "    cfgfile = \"cfg/yolo.cfg\"\n",
    "\n",
    "else:\n",
    "    print(\"Invalid dataset\")\n",
    "    exit()\n",
    "\n",
    "stride = 32\n",
    "bbox_attrs = 5 + num_classes\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(cfgfile)\n",
    "model.load_weights(weightsfile)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model(get_test_input(inp_dim, CUDA).to(\"cuda:0\" if CUDA else \"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "videofile = \"../Videos/V1.avi\"\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "frames = 0\n",
    "start = time.time()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.resize(frame, (int(frame.shape[1] / 2), int(frame.shape[0] / 2)))\n",
    "        img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "        im_dim = torch.FloatTensor(dim).repeat(1, 2)\n",
    "\n",
    "        if CUDA:\n",
    "            im_dim = im_dim.cuda()\n",
    "\n",
    "        output = model(Variable(img.to(\"cuda:0\" if CUDA else \"cpu\"), volatile=True)).data\n",
    "        output = predict_transform(output, inp_dim, stride, model.anchors, num_classes, confidence, CUDA)\n",
    "\n",
    "        if type(output) == int:\n",
    "            frames += 1\n",
    "            print(\"FPS of the video is {:5.2f}\".format(frames / (time.time() - start)))\n",
    "            cv2.imshow(\"frame\", orig_im)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                break\n",
    "            # continue\n",
    "\n",
    "        output = output.float()\n",
    "        # output = write_results(output, num_classes, nms=True, nms_conf=nms_thresh)\n",
    "\n",
    "# Check shape of output tensor\n",
    "        print(\"Shape of output tensor:\", output.shape)\n",
    "        \n",
    "        output = write_results(output, num_classes, nms=True, nms_conf=nms_thresh)\n",
    "        print(output.shape)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.unsqueeze(0)\n",
    "\n",
    "        output[:, 1:5] = torch.clamp(output[:, 1:5], 0.0, float(inp_dim))\n",
    "        # output[:, 1:5] = torch.clamp(output[:, 1:5], 0.0, float(inp_dim))\n",
    "\n",
    "        im_dim = im_dim.repeat(output.size(0), 1) / inp_dim\n",
    "        output[:, 1:5] *= im_dim\n",
    "\n",
    "        classes = load_classes('data/voc.names')\n",
    "        colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "        list(map(lambda x: write(x, orig_im, classes, colors), output))\n",
    "\n",
    "        cv2.imshow(\"frame\", orig_im)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "        frames += 1\n",
    "        print(\"FPS of the video is {:5.2f}\".format(frames / (time.time() - start)))\n",
    "\n",
    "    else:\n",
    "        print(\"End of video steam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
